// src/api/admin/generate-conversation-summary/route.ts
import { MedusaRequest, MedusaResponse } from '@medusajs/framework';
import OpenAI from 'openai';

// Inicializar OpenAI
const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY!
});

interface ChatMessage {
    id: string;
    user_id: string;
    message: string;
    role: "user" | "assistant";
    created_at: string;
    status: "IA" | "AGENTE";
    conversation_id?: string;
}

interface SummaryRequestBody {
    messages: ChatMessage[];
    userId: string;
}

// Funci√≥n para generar resumen usando OpenAI
async function generateConversationSummary(messages: ChatMessage[]): Promise<string> {
    try {
        // Filtrar mensajes del usuario
        const userMessages = messages
            .filter(msg => msg.role === 'user')
            .map(msg => msg.message)
            .join('\n\n');

        // Obtener respuestas del asistente para contexto
        const assistantMessages = messages
            .filter(msg => msg.role === 'assistant')
            .slice(-5) // √öltimas 5 respuestas para contexto
            .map(msg => msg.message)
            .join('\n\n');

        // Detectar si hay productos o servicios mencionados
        const productMentions = messages
            .filter(msg => msg.role === 'user')
            .map(msg => msg.message)
            .join(' ')
            .match(/patinete|scooter|bateria|motor|rueda|freno|casco|repuesto|pieza|modelo|marca/gi) || [];

        // Detectar n√∫meros de pedido
        const orderNumbers = messages
            .filter(msg => msg.role === 'user')
            .map(msg => msg.message)
            .join(' ')
            .match(/\d{5}/g) || [];

        const prompt = `
Eres un asistente especializado en an√°lisis de conversaciones de soporte t√©cnico para una empresa de patinetes el√©ctricos.

Analiza la siguiente conversaci√≥n y genera un resumen estructurado:

**MENSAJES DEL USUARIO:**
${userMessages}

**RESPUESTAS DEL SISTEMA (para contexto):**
${assistantMessages}

**PRODUCTOS DETECTADOS:** ${productMentions.join(', ') || 'Ninguno espec√≠fico'}
**N√öMEROS DE PEDIDO DETECTADOS:** ${orderNumbers.join(', ') || 'Ninguno'}

Genera un resumen siguiendo EXACTAMENTE este formato:

## üìã RESUMEN DE CONVERSACI√ìN

**üîç Problema Principal:**
[Descripci√≥n clara y concisa del problema principal del usuario en 1-2 l√≠neas]

**üìù Detalles Importantes:**
[Informaci√≥n relevante: productos mencionados, n√∫meros de pedido, s√≠ntomas espec√≠ficos, etc.]

**üìä Contexto de la Consulta:**
[Historial previo mencionado, intentos de soluci√≥n realizados, estado actual]

**‚ö° Nivel de Urgencia:**
[üü¢ BAJA / üü° MEDIA / üî¥ ALTA] - [Justificaci√≥n breve]

**üéØ Acciones Recomendadas:**
‚Ä¢ [Acci√≥n espec√≠fica 1]
‚Ä¢ [Acci√≥n espec√≠fica 2]
‚Ä¢ [Acci√≥n espec√≠fica 3 si es necesaria]

**üí¨ Tono del Usuario:**
[Calmado/Preocupado/Frustrado/Urgente] - [Observaciones sobre la actitud]

**üîÑ Estado de Resoluci√≥n:**
[Pendiente/Parcialmente resuelto/Requiere escalamiento/Informaci√≥n adicional necesaria]

Mant√©n cada secci√≥n concisa pero informativa. Si no hay informaci√≥n suficiente para una secci√≥n, indica "No especificado" o "No aplica".
`;

        const completion = await openai.chat.completions.create({
            model: "gpt-3.5-turbo",
            messages: [
                {
                    role: "system",
                    content: "Eres un especialista en an√°lisis de conversaciones de soporte t√©cnico. Genera res√∫menes estructurados, claros y accionables para agentes de soporte."
                },
                {
                    role: "user",
                    content: prompt
                }
            ],
            max_tokens: 800,
            temperature: 0.2, // Baja temperatura para respuestas m√°s consistentes
            presence_penalty: 0.1,
            frequency_penalty: 0.1
        });

        return completion.choices[0]?.message?.content || "‚ùå No se pudo generar el resumen.";

    } catch (error) {
        console.error("Error generando resumen con OpenAI:", error);
        throw new Error(`Error del servicio de IA: ${error instanceof Error ? error.message : 'Error desconocido'}`);
    }
}

// Handler POST para generar resumen
export async function POST(
    req: MedusaRequest,
    res: MedusaResponse
): Promise<void> {
    const startTime = Date.now();
    
    try {
        const { messages, userId } = req.body as SummaryRequestBody;

        // Validaciones de entrada
        if (!messages || !Array.isArray(messages)) {
            res.status(400).json({
                success: false,
                error: 'El campo "messages" es requerido y debe ser un array',
                code: 'INVALID_MESSAGES'
            });
            return;
        }

        if (messages.length === 0) {
            res.status(400).json({
                success: false,
                error: 'Se requiere al menos un mensaje para generar el resumen',
                code: 'EMPTY_MESSAGES'
            });
            return;
        }

        if (!userId || typeof userId !== 'string') {
            res.status(400).json({
                success: false,
                error: 'El campo "userId" es requerido',
                code: 'INVALID_USER_ID'
            });
            return;
        }

        // Validar que el API key de OpenAI est√© configurado
        if (!process.env.OPENAI_API_KEY) {
            console.error('‚ùå OPENAI_API_KEY no est√° configurado en las variables de entorno');
            res.status(500).json({
                success: false,
                error: 'Servicio de IA no configurado',
                code: 'AI_SERVICE_NOT_CONFIGURED'
            });
            return;
        }

        console.log(`üîÑ Iniciando generaci√≥n de resumen para usuario: ${userId}`);
        console.log(`üìä Total de mensajes a analizar: ${messages.length}`);
        console.log(`üë§ Mensajes de usuario: ${messages.filter(m => m.role === 'user').length}`);
        console.log(`ü§ñ Mensajes de asistente: ${messages.filter(m => m.role === 'assistant').length}`);

        // Generar el resumen
        const summary = await generateConversationSummary(messages);
        const processingTime = Date.now() - startTime;

        console.log(`‚úÖ Resumen generado exitosamente en ${processingTime}ms para usuario: ${userId}`);

        // Respuesta exitosa
        res.status(200).json({
            success: true,
            summary,
            metadata: {
                userId,
                generatedAt: new Date().toISOString(),
                messageCount: messages.length,
                userMessageCount: messages.filter(m => m.role === 'user').length,
                assistantMessageCount: messages.filter(m => m.role === 'assistant').length,
                processingTimeMs: processingTime
            }
        });

    } catch (error) {
        const processingTime = Date.now() - startTime;
        console.error(`‚ùå Error generando resumen de conversaci√≥n (${processingTime}ms):`, error);

        // Determinar el tipo de error y respuesta apropiada
        if (error instanceof Error && error.message.includes('API key')) {
            res.status(401).json({
                success: false,
                error: 'Error de autenticaci√≥n con el servicio de IA',
                code: 'AI_AUTH_ERROR'
            });
            return;
        }

        if (error instanceof Error && error.message.includes('rate limit')) {
            res.status(429).json({
                success: false,
                error: 'L√≠mite de uso del servicio de IA alcanzado. Int√©ntelo m√°s tarde.',
                code: 'AI_RATE_LIMIT'
            });
            return;
        }

        if (error instanceof Error && error.message.includes('timeout')) {
            res.status(408).json({
                success: false,
                error: 'El servicio de IA tard√≥ demasiado en responder',
                code: 'AI_TIMEOUT'
            });
            return;
        }

        // Error gen√©rico
        res.status(500).json({
            success: false,
            error: 'Error interno del servidor al generar el resumen',
            code: 'INTERNAL_ERROR',
            details: process.env.NODE_ENV === 'development' ? 
                (error instanceof Error ? error.message : 'Error desconocido') : undefined,
            processingTimeMs: processingTime
        });
    }
}

// Opcional: Handler GET para verificar que el endpoint est√° funcionando
export async function GET(
    req: MedusaRequest,
    res: MedusaResponse
): Promise<void> {
    res.status(200).json({
        message: "Endpoint de resumen de conversaciones activo",
        version: "1.0.0",
        methods: ["POST"],
        endpoint: "/admin/conversation-summary"
    });
}